{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись файла для сдачи на Stepik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile homework02.py\n",
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import lightning as pl\n",
    "from lightning import Trainer, LightningModule, LightningDataModule# Джентельменский наборчик\n",
    "from lightning.pytorch.callbacks import  ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# your code here\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 2025\n",
    "    accelerator: str = \"auto\"  # Автоматическое определение\n",
    "    devices: int = 1\n",
    "    dropout: float = 0.3\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 200\n",
    "    test_size: float = 0.2\n",
    "    num_workers: int = min(2, os.cpu_count())\n",
    "    epochs: int = 10\n",
    "    gpus: int = 2\n",
    "    stride: int = 1\n",
    "    dilation: int =1\n",
    "    n_classes: int =25\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        pl.seed_everything(self.seed)\n",
    "        # Автоматическое определение доступных устройств\n",
    "        if torch.cuda.is_available():\n",
    "            self.accelerator = \"gpu\"\n",
    "            self.devices = torch.cuda.device_count()\n",
    "        else:\n",
    "            self.accelerator = \"cpu\"\n",
    "            self.devices = 1\n",
    "\n",
    "# Dataset\n",
    "class SignLanguageDataset(data.Dataset):\n",
    "\n",
    "    \n",
    "    def __init__(self, df, transform: Optional[transforms.Compose] = None, is_train: bool = True):\n",
    "        \n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        label = self.df.iloc[index, 0]\n",
    "        \n",
    "        img = self.df.iloc[index, 1:].values.reshape(28, 28)\n",
    "        img = torch.Tensor(img).unsqueeze(0)\n",
    "        if self.transform is not None:\n",
    "            img =self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Data Module\n",
    "class SignLanguageDatasetLightning(LightningDataModule):\n",
    "    \n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.train_transform = transforms.Compose([\n",
    "        #transforms.Normalize(159, 40),\n",
    "        transforms.RandomHorizontalFlip(p=0.1),\n",
    "        transforms.RandomApply([transforms.RandomRotation(degrees=(-180, 180))], p=0.2),\n",
    "    ])\n",
    "            \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download and prepare data (called once)\"\"\"\n",
    "        self.train_df = self._read_zip_csv(\n",
    "            \"https://github.com/a-milenkin/ml_instruments/raw/main/data/sign_mnist_train.csv.zip\"\n",
    "        )\n",
    "        self.test_df = self._read_zip_csv(\n",
    "            \"https://github.com/a-milenkin/ml_instruments/raw/main/data/sign_mnist_test.csv.zip\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_df, val_df = train_test_split(\n",
    "                self.train_df, \n",
    "                test_size=self.cfg.test_size,\n",
    "                stratify=self.train_df.iloc[:, 0]\n",
    "            )\n",
    "            \n",
    "            self.train = SignLanguageDataset(train_df, self.train_transform)\n",
    "            self.val = SignLanguageDataset(val_df, is_train=False)\n",
    "            \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test = SignLanguageDataset(self.test_df, is_train=False)\n",
    "                     \n",
    "    def _make_dataloader(self, dataset):\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def _read_zip_csv(self, url: str) -> pd.DataFrame:\n",
    "        \"\"\"Helper to read CSV from ZIP\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise\n",
    "\n",
    "        with ZipFile(BytesIO(response.content)) as zip_file:\n",
    "            csv_files = [f for f in zip_file.namelist() \n",
    "                        if f.endswith('.csv') and '__MACOSX' not in f]\n",
    "            if len(csv_files) != 1:\n",
    "                raise ValueError(f\"Expected 1 CSV, found {len(csv_files)}\")\n",
    "                \n",
    "            with zip_file.open(csv_files[0]) as f:\n",
    "                return pd.read_csv(f)\n",
    "            \n",
    "    def train_dataloader(self)-> DataLoader:\n",
    "        # Возвращаем Train Dataloader\n",
    "        return self._make_dataloader(self.train)\n",
    "    \n",
    "    def val_dataloader(self)-> DataLoader:\n",
    "        # Возвращаем Valid Dataset\n",
    "        return self._make_dataloader(self.val)\n",
    "        \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self._make_dataloader(self.test)\n",
    "    \n",
    "# Model\n",
    "class MyConvNetLightning(LightningModule):\n",
    "    \n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.lr = cfg.lr\n",
    "\n",
    "        self.stride = cfg.stride\n",
    "        self.dilation = cfg.dilation\n",
    "        self.n_classes = cfg.n_classes\n",
    "\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #input=(batch, 1, 28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1, stride=self.stride, dilation=self.dilation),\n",
    "            nn.BatchNorm2d(8),\n",
    "            # (batch, 8, 28, 28)\n",
    "            nn.AvgPool2d(2),\n",
    "            # (batch, 8, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=self.stride, dilation=self.dilation),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # (batch, 16, 14, 14)\n",
    "            nn.AvgPool2d(2),\n",
    "            # (batch, 16, 7, 7)\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features=16*7*7, out_features=100)\n",
    "        # (batch, 100)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.drop1 = nn.Dropout(p=cfg.dropout)\n",
    "        self.lin2 = nn.Linear(100, self.n_classes)\n",
    "        # (batch, 25)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        x = x.view((x.shape[0], -1))\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _shared_step(self, batch: tuple, step: str):\n",
    "        # получаем данные\n",
    "        data, target = batch\n",
    "        \n",
    "        pred = self(data)\n",
    "        \n",
    "        loss= self.criterion(pred, target)\n",
    "        acc = (pred.argmax(dim=1) == target).float().mean()\n",
    "        \n",
    "        loss_dict = {\n",
    "            f\"{step}/loss\": loss,\n",
    "            f\"{step}/acc\": acc\n",
    "        }\n",
    "        \n",
    "        self.log_dict(loss_dict, prog_bar=True)\n",
    "        \n",
    "        if step == \"val\":\n",
    "            self.log(\"val/acc\", acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._shared_step(batch,  \"train\")\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._shared_step(batch,\"val\")\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx ):\n",
    "        loss = self._shared_step(batch, \"test\")\n",
    "        return loss\n",
    "     \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=2, # Сколько эпох ждать без улучшения\n",
    "            verbose=True, # Печатать, когда LR меняется\n",
    "            factor=0.5, # Насколько уменьшить LR (в 2 раза, 0.1 = в 10 раз)\n",
    "            mode='min', # Что хотим минимизировать (например, val_loss)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\" # Должно совпадать с логгируемым именем\n",
    "            }\n",
    "        }\n",
    "    \n",
    "def main(args, cfg):\n",
    "\n",
    "    dataset = SignLanguageDatasetLightning(cfg)\n",
    "    dataset.prepare_data()\n",
    "    dataset.setup(\"fit\")\n",
    "    \n",
    "    model = MyConvNetLightning(cfg)\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        filename=\"best-{epoch}-{val_acc:.2f}\",\n",
    "        save_top_k=1\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=cfg.epochs, \n",
    "        log_every_n_steps=1,\n",
    "        devices=cfg.devices if cfg.accelerator == \"gpu\" else \"auto\",\n",
    "        # отключение чекпоинтов в режиме разработки\n",
    "        enable_checkpointing=not args.fast_dev_run,\n",
    "        fast_dev_run=args.fast_dev_run\n",
    "        \n",
    "    )\n",
    "    \n",
    "    if args.fast_dev_run:\n",
    "        try:\n",
    "#             trainer.test(datamodule=dataset, ckpt_path=\"best\")\n",
    "            trainer.validate(model, datamodule=dataset)\n",
    "            print(\"Тестовый прогон успешно пройден\")\n",
    "        except Exception as err:\n",
    "            print(f\"Тестовый прогон завершился с ошибкой {err}\")\n",
    "            return\n",
    "    else:\n",
    "        # Полный цикл обучения + тестирование\n",
    "        trainer.fit(model, datamodule=dataset)\n",
    "        trainer.test(model, datamodule=dataset) \n",
    "        \n",
    "    \n",
    "    # Инференс на одном образце\n",
    "    dataset.setup(\"test\")\n",
    "    model.eval()\n",
    "    sample_img, true_label = dataset.test[0]\n",
    "    sample = sample_img.unsqueeze(0)  # берём первый образец из теста\n",
    "    with torch.no_grad():\n",
    "        pred = model(sample)\n",
    "        predicted_class = pred.argmax(dim=1).item()\n",
    "        print(f\"Инференс на одном примере: Fact: {true_label}, Prediction:{predicted_class}\")\n",
    "\n",
    "        # Сохраним модель\n",
    "    state = {'model': model.state_dict(),\n",
    "            'epoch': 20}\n",
    "\n",
    "    model_path = Path('../models')\n",
    "    model_name = 'myconvnet_sign_lang.pth'\n",
    "    model_path.mkdir(parents=True, exist_ok=True) # Создаем папку, если ее не существует\n",
    "\n",
    "    torch.save(state, model_path / model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # your code here\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # определён через action=\"store_true\". При таком способе использования флаг устанавливается в True\n",
    "    # , если он присутствует, и не требует передачи дополнительного значения. \n",
    "    # То есть, для включения режима fast_dev_run достаточно указать просто флаг без параметров.\n",
    "    parser.add_argument(\"--fast_dev_run\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "                                                                                       \n",
    "    cfg = CFG()\n",
    "  \n",
    "    main(args, cfg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
